{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Main.GraphDifferention"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "module GraphDifferention\n",
    "\n",
    "    using ExportAll\n",
    "    using LinearAlgebra:diagm\n",
    "\n",
    "    abstract type GraphNode end\n",
    "    abstract type Operator <: GraphNode end\n",
    "\n",
    "    #=\n",
    "        Structure types\n",
    "    =#\n",
    "\n",
    "    struct Constant{T} <: GraphNode\n",
    "        output :: T\n",
    "    end\n",
    "\n",
    "    mutable struct Variable <: GraphNode\n",
    "        output :: Any\n",
    "        gradient :: Any\n",
    "        name :: String\n",
    "        Variable(output; name=\"?\") = new(output, nothing, name)\n",
    "    end\n",
    "\n",
    "    mutable struct ScalarOperator{F} <: Operator\n",
    "        inputs :: Any\n",
    "        output :: Any\n",
    "        gradient :: Any\n",
    "        name :: String\n",
    "        ScalarOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, nothing, nothing, name)\n",
    "    end\n",
    "\n",
    "    mutable struct BroadcastedOperator{F} <: Operator\n",
    "        inputs :: Any\n",
    "        output :: Any\n",
    "        gradient :: Any\n",
    "        name :: String\n",
    "        BroadcastedOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, nothing, nothing, name)\n",
    "    end\n",
    "\n",
    "    ### Pretty-printing\n",
    "    ## It helps tracking what happens\n",
    "\n",
    "    import Base: show, summary\n",
    "\n",
    "    show(io::IO, x::ScalarOperator{F}) where {F} = print(io, \"op \", x.name, \"(\", F, \")\");\n",
    "    show(io::IO, x::BroadcastedOperator{F}) where {F} = print(io, \"op.\", x.name, \"(\", F, \")\");\n",
    "    show(io::IO, x::Constant) = print(io, \"const \", x.output)\n",
    "    show(io::IO, x::Variable) = begin\n",
    "        print(io, \"var \", x.name);\n",
    "        print(io, \"\\n ┣━ ^ \"); summary(io, x.output)\n",
    "        print(io, \"\\n ┗━ ∇ \");  summary(io, x.gradient)\n",
    "    end\n",
    "\n",
    "    ### Graph building\n",
    "\n",
    "    function visit(node::GraphNode, visited, order)\n",
    "        if node ∈ visited\n",
    "        else\n",
    "            push!(visited, node)\n",
    "            push!(order, node)\n",
    "        end\n",
    "        return nothing\n",
    "    end\n",
    "        \n",
    "    function visit(node::Operator, visited, order)\n",
    "        if node ∈ visited\n",
    "        else\n",
    "            push!(visited, node)\n",
    "            for input in node.inputs\n",
    "                visit(input, visited, order)\n",
    "            end\n",
    "            push!(order, node)\n",
    "        end\n",
    "        return nothing\n",
    "    end\n",
    "    \n",
    "    function topological_sort(head::GraphNode)\n",
    "        visited = Set()\n",
    "        order = Vector()\n",
    "        visit(head, visited, order)\n",
    "        return order\n",
    "    end\n",
    "\n",
    "    ### Forward pass\n",
    "\n",
    "    reset!(node::Constant) = nothing\n",
    "    reset!(node::Variable) = node.gradient = nothing\n",
    "    reset!(node::Operator) = node.gradient = nothing\n",
    "\n",
    "    compute!(node::Constant) = nothing\n",
    "    compute!(node::Variable) = nothing\n",
    "    compute!(node::Operator) =\n",
    "        node.output = forward(node, [input.output for input in node.inputs]...)\n",
    "\n",
    "    function forward!(order::Vector)\n",
    "        for node in order\n",
    "            compute!(node)\n",
    "            reset!(node)\n",
    "        end\n",
    "        return last(order).output\n",
    "    end\n",
    "\n",
    "    ### Backward pass\n",
    "\n",
    "    update!(node::Constant, gradient) = nothing\n",
    "    update!(node::GraphNode, gradient) = if isnothing(node.gradient)\n",
    "        node.gradient = gradient else node.gradient .+= gradient\n",
    "    end\n",
    "\n",
    "    function backward!(order::Vector, seed = ones(length(last(order).output)))\n",
    "        result = last(order)\n",
    "        result.gradient = seed\n",
    "        #@assert length(result.output) == 1 \"Gradient is defined only for scalar functions\"\n",
    "        for node in reverse(order)\n",
    "            backward!(node)\n",
    "        end\n",
    "        return nothing\n",
    "    end\n",
    "\n",
    "    function backward!(node::Constant) end\n",
    "    function backward!(node::Variable) end\n",
    "    function backward!(node::Operator)\n",
    "        inputs = node.inputs\n",
    "        gradients = backward(node, [input.output for input in inputs]..., node.gradient)\n",
    "        for (input, gradient) in zip(inputs, gradients)\n",
    "            update!(input, gradient)\n",
    "        end\n",
    "        return nothing\n",
    "    end\n",
    "\n",
    "    # Scalar operators\n",
    "\n",
    "    import Base: ^\n",
    "    ^(x::GraphNode, n::GraphNode) = ScalarOperator(^, x, n)\n",
    "    ^(x::GraphNode, n::Number) = ScalarOperator(^, x, Constant(n))\n",
    "    forward(::ScalarOperator{typeof(^)}, x, n) = return x^n\n",
    "    backward(::ScalarOperator{typeof(^)}, x, n, g) = tuple(g * n * x ^ (n-1), g * log(abs(x)) * x ^ n)\n",
    "\n",
    "    import Base: sin\n",
    "    sin(x::GraphNode) = ScalarOperator(sin, x)\n",
    "    forward(::ScalarOperator{typeof(sin)}, x) = return sin(x)\n",
    "    backward(::ScalarOperator{typeof(sin)}, x, g) = tuple(g * cos(x))\n",
    "    \n",
    "    import Base: tanh\n",
    "    tanh(x::GraphNode) = ScalarOperator(tanh,x)\n",
    "    forward(::ScalarOperator{typeof(tanh)}, x) = return tanh.(x)\n",
    "    backward(node::ScalarOperator{typeof(tanh)}, x, g) = let\n",
    "        𝟏 = ones(length(node.output))\n",
    "        x_sqr = node.output.^2\n",
    "        derivative_tanh_vector = 𝟏 .- x_sqr\n",
    "        tuple(derivative_tanh_vector .* g)\n",
    "    end\n",
    "    # Broadcast operators\n",
    "\n",
    "    import Base: *\n",
    "    import LinearAlgebra: mul!\n",
    "    # x * y (aka matrix multiplication)\n",
    "    *(A::GraphNode, x::GraphNode) = BroadcastedOperator(mul!, A, x)\n",
    "    forward(::BroadcastedOperator{typeof(mul!)}, A, x) = return A * x\n",
    "    backward(::BroadcastedOperator{typeof(mul!)}, A, x, g) = tuple(g * x', A' * g)\n",
    "\n",
    "    # x .* y (element-wise multiplication)\n",
    "    Base.Broadcast.broadcasted(*, x::GraphNode, y::GraphNode) = BroadcastedOperator(*, x, y)\n",
    "    forward(::BroadcastedOperator{typeof(*)}, x, y) = return x .* y\n",
    "    backward(node::BroadcastedOperator{typeof(*)}, x, y, g) = let\n",
    "        𝟏 = ones(length(node.output))\n",
    "        Jx = diagm(y .* 𝟏)\n",
    "        Jy = diagm(x .* 𝟏)\n",
    "        tuple(Jx' * g, Jy' * g)\n",
    "    end\n",
    "\n",
    "    softmax(x) = let     \n",
    "        shiftx = x .- maximum(x)\n",
    "        exps = exp.(shiftx)\n",
    "        return exps ./ sum(exps)\n",
    "    end\n",
    "    \n",
    "    softmax(x::GraphNode) = BroadcastedOperator(softmax,x)\n",
    "    forward(::BroadcastedOperator{typeof(softmax)},x) = return softmax(x)\n",
    "    backward(node::BroadcastedOperator{typeof(softmax)},x,g)  = let\n",
    "        vector_of_derivatives = Vector()\n",
    "        for i in 1:length(node.output)\n",
    "            yi = node.output[i]\n",
    "            ∑dLdYj_times_Yj = sum(g.* node.output)\n",
    "            dLdYi = g[i]\n",
    "            result  =  -yi*(∑dLdYj_times_Yj - dLdYi)\n",
    "            push!(vector_of_derivatives,result)\n",
    "        end\n",
    "        tuple(vector_of_derivatives)\n",
    "    end\n",
    "\n",
    "    crossentropy(output,target) =  sum(-target.*log.(output))\n",
    "    crossentropy(x::GraphNode,y::GraphNode) = BroadcastedOperator(crossentropy,x,y)\n",
    "    forward(::BroadcastedOperator{typeof(GraphDifferention.crossentropy)},x,y) = return crossentropy(x,y)\n",
    "    backward(node::BroadcastedOperator{typeof(GraphDifferention.crossentropy)},x,y,g)  = let\n",
    "        𝟏 = ones(length(x))\n",
    "        tuple(g.*(-(y./x) + (𝟏.-y)./(𝟏.-x)))\n",
    "    end\n",
    "\n",
    "\n",
    "    Base.Broadcast.broadcasted(-, x::GraphNode, y::GraphNode) = BroadcastedOperator(-, x, y)\n",
    "    forward(::BroadcastedOperator{typeof(-)}, x, y) = return x .- y\n",
    "    backward(::BroadcastedOperator{typeof(-)}, x, y, g) = tuple(g,-g)\n",
    "\n",
    "    Base.Broadcast.broadcasted(+, x::GraphNode, y::GraphNode) = BroadcastedOperator(+, x, y)\n",
    "    forward(::BroadcastedOperator{typeof(+)}, x, y) = return x .+ y\n",
    "    backward(::BroadcastedOperator{typeof(+)}, x, y, g) = tuple(g, g)\n",
    "\n",
    "    import Base: sum\n",
    "    sum(x::GraphNode) = BroadcastedOperator(sum, x)\n",
    "    forward(::BroadcastedOperator{typeof(sum)}, x) = return sum(x)\n",
    "    backward(::BroadcastedOperator{typeof(sum)}, x, g) = let\n",
    "        𝟏 = ones(length(x))\n",
    "        J = 𝟏'\n",
    "        tuple(J' * g)\n",
    "    end\n",
    "\n",
    "    Base.Broadcast.broadcasted(/, x::GraphNode, y::GraphNode) = BroadcastedOperator(/, x, y)\n",
    "    forward(::BroadcastedOperator{typeof(/)}, x, y) = return x ./ y\n",
    "    backward(node::BroadcastedOperator{typeof(/)}, x, y::Real, g) = let\n",
    "        𝟏 = ones(length(node.output))\n",
    "        Jx = diagm(𝟏 ./ y)\n",
    "        Jy = (-x ./ y .^2)\n",
    "        tuple(Jx' * g, Jy' * g)\n",
    "    end\n",
    "\n",
    "    import Base: max\n",
    "    Base.Broadcast.broadcasted(max, x::GraphNode, y::GraphNode) = BroadcastedOperator(max, x, y)\n",
    "    forward(::BroadcastedOperator{typeof(max)}, x, y) = return max.(x, y)\n",
    "    backward(::BroadcastedOperator{typeof(max)}, x, y, g) = let\n",
    "        Jx = diagm(isless.(y, x))\n",
    "        Jy = diagm(isless.(x, y))\n",
    "        tuple(Jx' * g, Jy' * g)\n",
    "    end\n",
    "\n",
    "    @exportAll()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Main.AutomaticDifferention"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "module AutomaticDifferention\n",
    "\n",
    "    using ..GraphDifferention\n",
    "    using ExportAll\n",
    "    using Distributions\n",
    "    using LinearAlgebra:I\n",
    "\n",
    "\n",
    "    abstract type NetworkLayer end\n",
    "\n",
    "    struct DenseLayerSoftmax <: NetworkLayer\n",
    "        ordered_computation_graph :: Vector{GraphNode}\n",
    "        w_handle :: GraphNode\n",
    "        x_handle :: GraphNode\n",
    "        output_handle :: GraphNode\n",
    "        prediction_handle :: GraphNode\n",
    "        DenseLayerSoftmax(input_output_pair,test_data) = let \n",
    "            \n",
    "            input_num = input_output_pair.first\n",
    "            output_num = input_output_pair.second\n",
    "\n",
    "            @assert length(test_data) == output_num\n",
    "\n",
    "            ordered_graph, w_handle, x_handle, output_handle, prediction_handle = constructSoftmaxDense(input_num,output_num,test_data)\n",
    "            new(ordered_graph,w_handle,x_handle,output_handle, prediction_handle)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    struct RnnVanillaTanh <: NetworkLayer\n",
    "        ordered_computation_graph :: Vector{GraphNode}\n",
    "        w_handle :: GraphNode\n",
    "        u_handle :: GraphNode\n",
    "        x_handle :: GraphNode\n",
    "        h_handle :: GraphNode\n",
    "        output_handle :: GraphNode\n",
    "        RnnVanillaTanh(input_output_pair) = let \n",
    "            \n",
    "            input_num = input_output_pair.first\n",
    "            output_num = input_output_pair.second\n",
    "\n",
    "            ordered_graph, h_handle, w_handle, u_handle, x_handle, output_handle = constructVanillaTanhRnn(input_num,output_num)\n",
    "            new(ordered_graph,w_handle,u_handle,x_handle,h_handle,output_handle)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    handle_batching_preperations!(layer::DenseLayerSoftmax) = println(\"Loss = $(layer.output_handle.output)\")\n",
    "    handle_batching_preperations!(layer::RnnVanillaTanh) = load_output_as_h!(layer)\n",
    "\n",
    "    function constructVanillaTanhRnn(input_number,outputs_number)\n",
    "        x = Variable(ones(input_number,1), name = \"x-rnn\")\n",
    "        u = Variable(rand(Uniform(-0.01,0.01),outputs_number,input_number), name = \"u-rnn\")\n",
    "\n",
    "        h = Variable(rand(Uniform(-0.01,0.01),outputs_number,1), name = \"h-rnn\")\n",
    "        w = Variable(rand(Uniform(-0.01,0.01),outputs_number,outputs_number), name = \"w-rnn\")\n",
    "\n",
    "        b = Constant(rand(Uniform(-0.01,0.01),outputs_number,1))\n",
    "        o = (u*x .+ w*h) .+ b\n",
    "\n",
    "        activation = tanh(o)\n",
    "        order = topological_sort(activation)\n",
    "\n",
    "        return order, h, w, u, x, last(order)\n",
    "    end\n",
    "\n",
    "    function constructSoftmaxDense(input_number,outputs_number,test_data)\n",
    "        b = Constant(rand(Uniform(-0.01,0.01),outputs_number,1))\n",
    "        x = Variable(ones(input_number,1), name = \"x-dense\")\n",
    "\n",
    "        w = Variable(rand(Uniform(-0.01,0.01),outputs_number,input_number), name = \"w-dense\")\n",
    "        test = Constant(test_data)\n",
    "\n",
    "        o = (w*x) .+ b\n",
    "        activation = softmax(o)\n",
    "        loss = crossentropy(activation,test)\n",
    "        order = topological_sort(loss)\n",
    "\n",
    "        return order, w, x, last(order), activation\n",
    "    end\n",
    "\n",
    "    function load_output_as_h!(layer::RnnVanillaTanh)\n",
    "        if !isnothing(layer.output_handle.output)\n",
    "            layer.h_handle.output = layer.output_handle.output\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    function run_through_batched_data!(batched_data,network)\n",
    "        for data_batch in batched_data\n",
    "            load_batch_of_data!(data_batch, network)\n",
    "        end\n",
    "        backward_net!(network...)\n",
    "        println(\"Gradient:\")\n",
    "        @show last(network).w_handle.gradient\n",
    "    end\n",
    "\n",
    "    function backward_net!(layers...)\n",
    "        @assert length(layers) > 1 \"This function can be run for at least two layers.\"\n",
    "        reversed_layers = reverse(layers)\n",
    "        last_layer = reversed_layers[1]\n",
    "        other_layers = reversed_layers[2:end]\n",
    "\n",
    "        backward!(last_layer.ordered_computation_graph)\n",
    "        gradient_from_last_layer = last_layer.x_handle.gradient\n",
    "\n",
    "        for iter in eachindex(other_layers)\n",
    "\n",
    "            backward!(other_layers[iter].ordered_computation_graph,gradient_from_last_layer)\n",
    "            current_layer_gradient = other_layers[iter].x_handle.gradient\n",
    "\n",
    "            if iter + 1 < length(other_layers)\n",
    "                backward!(other_layers[iter+1],current_layer_gradient)\n",
    "                gradient_from_last_layer = other_layers[iter+1].x_handle.gradient\n",
    "            end\n",
    "        end\n",
    "\n",
    "        return last(other_layers).output_handle.gradient\n",
    "    end\n",
    "\n",
    "    function load_batch_of_data!(input_batch,network)\n",
    "        for layer in network\n",
    "            handle_batching_preperations!(layer)\n",
    "        end\n",
    "        forward_net!(input_batch,network...)\n",
    "    end\n",
    "\n",
    "    function backward_net!(layer)\n",
    "        backward!(layer.ordered_computation_graph)\n",
    "        gradient_from_last_layer = layer.x_handle.gradient\n",
    "\n",
    "        return gradient_from_last_layer\n",
    "    end\n",
    "\n",
    "    function forward_net!(input, layers...)\n",
    "        @assert length(layers) > 1 \"This function can be run for at least two layers.\"\n",
    "        first_layer = layers[1]\n",
    "        other_layers = layers[2:end]\n",
    "        first_layer.x_handle.output = input\n",
    "\n",
    "        output_from_first_layer = forward!(first_layer.ordered_computation_graph)\n",
    "        other_layers[1].x_handle.output = output_from_first_layer\n",
    "\n",
    "        for iter in eachindex(other_layers)\n",
    "            \n",
    "            current_layer_output = forward!(other_layers[iter].ordered_computation_graph)\n",
    "\n",
    "            if iter + 1 < length(other_layers)\n",
    "                other_layers[iter+1].x_handle.output = current_layer_output\n",
    "            end\n",
    "        end\n",
    "\n",
    "        return last(other_layers).output_handle.output\n",
    "    end\n",
    "\n",
    "    function forward_net!(input, layer)\n",
    "        layer.x_handle.output = input\n",
    "\n",
    "        output_from_layer = forward!(layer.ordered_computation_graph)\n",
    "\n",
    "        return output_from_layer\n",
    "    end\n",
    "\n",
    "    function learning_step(xᵢ, ∇fxᵢ, α = 0.001)\n",
    "        # steepest descent\n",
    "        xᵢ₊₁ = xᵢ + α*∇fxᵢ # this will be the new weight matrix\n",
    "        return xᵢ₊₁\n",
    "    end\n",
    "\n",
    "    update_learnin_step!(layer::DenseLayerSoftmax) = let \n",
    "        ∇w = layer.w_handle.gradient;\n",
    "        w = layer.w_handle.output;\n",
    "        layer.w_handle.output = learning_step(w,∇w)\n",
    "    end\n",
    "\n",
    "    update_learnin_step!(layer::RnnVanillaTanh) = let \n",
    "        ∇w = layer.w_handle.gradient;\n",
    "        w = layer.w_handle.output;\n",
    "        layer.w_handle.output = learning_step(w,∇w)\n",
    "\n",
    "        ∇h = layer.h_handle.gradient;\n",
    "        h = layer.h_handle.output;\n",
    "        layer.h_handle.output = learning_step(h,∇h)\n",
    "    end\n",
    "\n",
    "    function update_net_weights!(layers)\n",
    "        for layer in layers\n",
    "            update_learnin_step!(layer)\n",
    "        end\n",
    "    end\n",
    "    @exportAll\n",
    "end # module AutomaticDifferention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{NetworkLayer}:\n",
       " RnnVanillaTanh(Main.GraphDifferention.GraphNode[var u-rnn\n",
       " ┣━ ^ 64×196 Matrix{Float64}\n",
       " ┗━ ∇ Nothing, var x-rnn\n",
       " ┣━ ^ 196×1 Matrix{Float64}\n",
       " ┗━ ∇ Nothing, op.?(typeof(LinearAlgebra.mul!)), var w-rnn\n",
       " ┣━ ^ 64×64 Matrix{Float64}\n",
       " ┗━ ∇ Nothing, var h-rnn\n",
       " ┣━ ^ 64×1 Matrix{Float64}\n",
       " ┗━ ∇ Nothing, op.?(typeof(LinearAlgebra.mul!)), op.?(typeof(+)), const [-0.0010684570063299505; -0.0008121146893986225; … ; -0.009087544330006997; -0.008334445911758998;;], op.?(typeof(+)), op ?(typeof(tanh))], var w-rnn\n",
       " ┣━ ^ 64×64 Matrix{Float64}\n",
       " ┗━ ∇ Nothing, var u-rnn\n",
       " ┣━ ^ 64×196 Matrix{Float64}\n",
       " ┗━ ∇ Nothing, var x-rnn\n",
       " ┣━ ^ 196×1 Matrix{Float64}\n",
       " ┗━ ∇ Nothing, var h-rnn\n",
       " ┣━ ^ 64×1 Matrix{Float64}\n",
       " ┗━ ∇ Nothing, op ?(typeof(tanh)))\n",
       " DenseLayerSoftmax(Main.GraphDifferention.GraphNode[var w-dense\n",
       " ┣━ ^ 10×64 Matrix{Float64}\n",
       " ┗━ ∇ Nothing, var x-dense\n",
       " ┣━ ^ 64×1 Matrix{Float64}\n",
       " ┗━ ∇ Nothing, op.?(typeof(LinearAlgebra.mul!)), const [0.008141210787035306; 0.00496599986824273; … ; -0.008898247369836146; 0.0012315466303741158;;], op.?(typeof(+)), op.?(typeof(Main.GraphDifferention.softmax)), const Bool[0, 0, 0, 0, 0, 1, 0, 0, 0, 0], op.?(typeof(Main.GraphDifferention.crossentropy))], var w-dense\n",
       " ┣━ ^ 10×64 Matrix{Float64}\n",
       " ┗━ ∇ Nothing, var x-dense\n",
       " ┣━ ^ 64×1 Matrix{Float64}\n",
       " ┗━ ∇ Nothing, op.?(typeof(Main.GraphDifferention.crossentropy)), op.?(typeof(Main.GraphDifferention.softmax)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using .AutomaticDifferention\n",
    "using MLDatasets, Flux\n",
    "using Statistics:mean\n",
    "\n",
    "train_data = MLDatasets.MNIST(split=:train)\n",
    "test_data  = MLDatasets.MNIST(split=:test)\n",
    "\n",
    "# Prepare data\n",
    "x1dim = reshape(train_data.features, 28 * 28, :)\n",
    "yhot  = Flux.onehotbatch(train_data.targets, 0:9)\n",
    "\n",
    "in1 = x1dim[:,1][1:196]\n",
    "in2 = x1dim[:,1][197:392]\n",
    "in3 = x1dim[:,1][393:588]\n",
    "in4 = x1dim[:,1][589:end]\n",
    "\n",
    "yhot1 = yhot[:,1]\n",
    "\n",
    "# Prepare network\n",
    "rnn = RnnVanillaTanh(196 => 64)\n",
    "dense = DenseLayerSoftmax(64 => 10, yhot1)\n",
    "network = [rnn, dense]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64×196 Matrix{Float64}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱       ⋮                        ⋮\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_through_batched_data!([in1, in2, in3, in4],network)\n",
    "@show rnn.u_handle.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64×64 Matrix{Float64}:\n",
       "  1.11044e-5   -1.56344e-5    4.89347e-6  …  -1.52271e-5    8.082e-6\n",
       " -9.24676e-5    0.00013019   -4.07485e-5      0.000126798  -6.72998e-5\n",
       "  8.29198e-5   -0.000116747   3.6541e-5      -0.000113706   6.03507e-5\n",
       "  2.24796e-6   -3.16502e-6    9.9063e-7      -3.08256e-6    1.63611e-6\n",
       " -1.15813e-6    1.6306e-6    -5.10366e-7      1.58812e-6   -8.42914e-7\n",
       " -0.000126822   0.000178559  -5.58877e-5  …   0.000173907  -9.23035e-5\n",
       "  7.97202e-5   -0.000112242   3.5131e-5      -0.000109318   5.8022e-5\n",
       " -0.000101836   0.000143379  -4.48768e-5      0.000139644  -7.4118e-5\n",
       "  5.77299e-5   -8.12808e-5    2.54404e-5     -7.91633e-5    4.2017e-5\n",
       " -7.35979e-5    0.000103622  -3.24331e-5      0.000100923  -5.35661e-5\n",
       "  ⋮                                       ⋱                \n",
       "  3.05268e-6   -4.29802e-6    1.34525e-6  …  -4.18605e-6    2.2218e-6\n",
       " -1.53544e-5    2.16183e-5   -6.76638e-6      2.10551e-5   -1.11753e-5\n",
       " -5.74262e-5    8.08532e-5   -2.53065e-5      7.87468e-5   -4.17959e-5\n",
       "  5.52662e-5   -7.78121e-5    2.43547e-5     -7.5785e-5     4.02239e-5\n",
       " -4.63995e-5    6.53282e-5   -2.04473e-5      6.36263e-5   -3.37705e-5\n",
       "  0.000104972  -0.000147796   4.62591e-5  …  -0.000143945   7.6401e-5\n",
       "  5.75915e-5   -8.1086e-5     2.53794e-5     -7.89735e-5    4.19163e-5\n",
       "  8.66734e-5   -0.000122032   3.81952e-5     -0.000118853   6.30827e-5\n",
       " -3.77009e-5    5.3081e-5    -1.6614e-5       5.16982e-5   -2.74395e-5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@show rnn.w_handle.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×64 Matrix{Any}:\n",
       " -0.00039551    0.00229417  -0.000599908  …  -0.00908922  -0.00274198\n",
       " -0.000394195   0.00228654  -0.000597912     -0.00905898  -0.00273286\n",
       " -0.000389247   0.00225784  -0.000590407     -0.00894528  -0.00269856\n",
       " -0.000387545   0.00224797  -0.000587825     -0.00890616  -0.00268676\n",
       " -0.000392614   0.00227737  -0.000595515     -0.00902266  -0.00272191\n",
       "  0.00351726   -0.020402     0.00533496   …   0.0808301    0.0243844\n",
       " -0.000388983   0.00225631  -0.000590007     -0.00893921  -0.00269673\n",
       " -0.000388356   0.00225267  -0.000589056     -0.0089248   -0.00269238\n",
       " -0.000388641   0.00225433  -0.000589489     -0.00893136  -0.00269436\n",
       " -0.00039217    0.0022748   -0.000594841     -0.00901245  -0.00271882"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@show dense.w_handle.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64×196 Matrix{Float64}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱       ⋮                        ⋮\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "update_net_weights!(network)\n",
    "@show rnn.u_handle.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64×64 Matrix{Float64}:\n",
       "  1.11044e-5   -1.56344e-5    4.89347e-6  …  -1.52271e-5    8.082e-6\n",
       " -9.24676e-5    0.00013019   -4.07485e-5      0.000126798  -6.72998e-5\n",
       "  8.29198e-5   -0.000116747   3.6541e-5      -0.000113706   6.03507e-5\n",
       "  2.24796e-6   -3.16502e-6    9.9063e-7      -3.08256e-6    1.63611e-6\n",
       " -1.15813e-6    1.6306e-6    -5.10366e-7      1.58812e-6   -8.42914e-7\n",
       " -0.000126822   0.000178559  -5.58877e-5  …   0.000173907  -9.23035e-5\n",
       "  7.97202e-5   -0.000112242   3.5131e-5      -0.000109318   5.8022e-5\n",
       " -0.000101836   0.000143379  -4.48768e-5      0.000139644  -7.4118e-5\n",
       "  5.77299e-5   -8.12808e-5    2.54404e-5     -7.91633e-5    4.2017e-5\n",
       " -7.35979e-5    0.000103622  -3.24331e-5      0.000100923  -5.35661e-5\n",
       "  ⋮                                       ⋱                \n",
       "  3.05268e-6   -4.29802e-6    1.34525e-6  …  -4.18605e-6    2.2218e-6\n",
       " -1.53544e-5    2.16183e-5   -6.76638e-6      2.10551e-5   -1.11753e-5\n",
       " -5.74262e-5    8.08532e-5   -2.53065e-5      7.87468e-5   -4.17959e-5\n",
       "  5.52662e-5   -7.78121e-5    2.43547e-5     -7.5785e-5     4.02239e-5\n",
       " -4.63995e-5    6.53282e-5   -2.04473e-5      6.36263e-5   -3.37705e-5\n",
       "  0.000104972  -0.000147796   4.62591e-5  …  -0.000143945   7.6401e-5\n",
       "  5.75915e-5   -8.1086e-5     2.53794e-5     -7.89735e-5    4.19163e-5\n",
       "  8.66734e-5   -0.000122032   3.81952e-5     -0.000118853   6.30827e-5\n",
       " -3.77009e-5    5.3081e-5    -1.6614e-5       5.16982e-5   -2.74395e-5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@show rnn.w_handle.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×64 Matrix{Any}:\n",
       " -0.00039551    0.00229417  -0.000599908  …  -0.00908922  -0.00274198\n",
       " -0.000394195   0.00228654  -0.000597912     -0.00905898  -0.00273286\n",
       " -0.000389247   0.00225784  -0.000590407     -0.00894528  -0.00269856\n",
       " -0.000387545   0.00224797  -0.000587825     -0.00890616  -0.00268676\n",
       " -0.000392614   0.00227737  -0.000595515     -0.00902266  -0.00272191\n",
       "  0.00351726   -0.020402     0.00533496   …   0.0808301    0.0243844\n",
       " -0.000388983   0.00225631  -0.000590007     -0.00893921  -0.00269673\n",
       " -0.000388356   0.00225267  -0.000589056     -0.0089248   -0.00269238\n",
       " -0.000388641   0.00225433  -0.000589489     -0.00893136  -0.00269436\n",
       " -0.00039217    0.0022748   -0.000594841     -0.00901245  -0.00271882"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@show dense.w_handle.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64×196 Matrix{Float64}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱       ⋮                        ⋮\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_through_batched_data!([in1, in2, in3, in4],network)\n",
    "\n",
    "@show rnn.u_handle.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64×64 Matrix{Float64}:\n",
       "  1.06865e-5   -1.5046e-5     4.71451e-6  …  -1.46488e-5    7.77346e-6\n",
       " -9.00499e-5    0.000126786  -3.9727e-5       0.000123438  -6.55032e-5\n",
       "  8.22988e-5   -0.000115872   3.63074e-5     -0.000112813   5.9865e-5\n",
       "  8.11535e-6   -1.1426e-5     3.58022e-6     -1.11243e-5    5.90319e-6\n",
       " -3.21213e-6    4.5225e-6    -1.41708e-6      4.40309e-6   -2.33653e-6\n",
       " -0.000131461   0.00018509   -5.79961e-5  …   0.000180203  -9.56261e-5\n",
       "  7.84105e-5   -0.000110398   3.4592e-5      -0.000107483   5.70366e-5\n",
       " -9.52004e-5    0.000134037  -4.19992e-5      0.000130498  -6.92498e-5\n",
       "  6.50935e-5   -9.16482e-5    2.8717e-5      -8.92284e-5    4.73497e-5\n",
       " -7.54611e-5    0.000106245  -3.32909e-5      0.00010344   -5.48912e-5\n",
       "  ⋮                                       ⋱                \n",
       " -2.45706e-6    3.45941e-6   -1.08397e-6  …   3.36807e-6   -1.78729e-6\n",
       " -9.35445e-6    1.31706e-5   -4.12687e-6      1.28228e-5   -6.80453e-6\n",
       " -6.06972e-5    8.54584e-5   -2.67775e-5      8.3202e-5    -4.41517e-5\n",
       "  5.9888e-5    -8.43191e-5    2.64205e-5     -8.20929e-5    4.35631e-5\n",
       " -4.94224e-5    6.95842e-5   -2.18035e-5      6.7747e-5    -3.59504e-5\n",
       "  0.000107258  -0.000151014   4.73188e-5  …  -0.000147027   7.80209e-5\n",
       "  6.19243e-5   -8.71861e-5    2.73189e-5     -8.48841e-5    4.50444e-5\n",
       "  7.70998e-5   -0.000108552   3.40138e-5     -0.000105686   5.60832e-5\n",
       " -4.06188e-5    5.71891e-5   -1.79196e-5      5.56791e-5   -2.95465e-5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@show rnn.w_handle.gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×64 Matrix{Any}:\n",
       " -0.00039542    0.00229354  -0.000599344  …  -0.00908875  -0.00274226\n",
       " -0.000394104   0.00228591  -0.00059735      -0.00905851  -0.00273314\n",
       " -0.000389157   0.00225722  -0.000589852     -0.0089448   -0.00269883\n",
       " -0.000387455   0.00224735  -0.000587272     -0.00890568  -0.00268703\n",
       " -0.000392524   0.00227675  -0.000594955     -0.00902219  -0.00272218\n",
       "  0.00351645   -0.0203964    0.00532994   …   0.0808258    0.0243868\n",
       " -0.000388893   0.00225569  -0.000589452     -0.00893874  -0.002697\n",
       " -0.000388266   0.00225205  -0.000588501     -0.00892432  -0.00269265\n",
       " -0.000388552   0.00225371  -0.000588934     -0.00893088  -0.00269463\n",
       " -0.00039208    0.00227417  -0.000594282     -0.00901198  -0.0027191"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@show dense.w_handle.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i = 1:5\n",
    "    run_through_batched_data!([in1, in2, in3, in4],network)\n",
    "    update_net_weights!(network)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.3",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
